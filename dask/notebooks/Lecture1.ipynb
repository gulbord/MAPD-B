{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of parallel and distributed computing with DASK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap on task parallelization (this time using Dask)\n",
    "\n",
    "We should now start to feel quite familiar with the programming pattern already discussed in the previous lectures when we discussed Hadood MapReduce and the Spark framework.\n",
    "\n",
    "Asides from the details of the API, which depend on the framework implementation, and from the inner structure of the job scheduling and resorce manager, most of the distributed computing frameworks and tools offer ways of parallelizing our jobs by means of Graph schedulers and Task optimizers.\n",
    "\n",
    "The workflow is mostly the same, no matter the tool used to reach it:\n",
    "- **Divide large datasets which won't fit into memory in a set of partitions**\n",
    "- **Plan the entire data processing before execution and optimize it to break it into smaller tasks**\n",
    "- **Schedule and distribute processing near the data, minimizing data movement (data locality)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Dask, we still have access to a set of dataset representations:\n",
    "- Dask bag (~ equivalent to the _Spark RDD_)\n",
    "- Dask array\n",
    "- Dask dataframe (~ equivalent to the _Spark DataFrame_)\n",
    "\n",
    "However, Dask is primarily a **scheduler** (written in Python instead of Scala like Spark \\[*\\]), which enables _lazy_ execution of python-like code for distributing it across multiple workers.\n",
    "\n",
    "\n",
    "_\\[*\\] Note that the fact that Dask's scheduler is written in Python does not necessarily make it better than Spark for our purposes. Python and Scala/Java have their own pros and cons. For example, due to the way Python handles and manages memory during application execution, Dask still has issues with unmanaged memory throughout an application's life cycle (for more information, see this [link](https://coiled.io/blog/tackling-unmanaged-memory-with-dask/))._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a cluster\n",
    "\n",
    "We can use Dask locally to leverage the multitasking/processing capabilities of our local machine, or we can set up a cluster and deploy a scheduler and multiple worker nodes.\n",
    "\n",
    "Once we set up a cluster, we can initialize a `Client` by providing it with the address of a scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dask distributed client\n",
    "from dask.distributed import Client\n",
    "\n",
    "# instantiate the client by providing \n",
    "# the address:port of the scheduler\n",
    "client = Client('dask-scheduler:8786')\n",
    "\n",
    "# inspect the client\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can monitor the status of the cluster with a dedicated webui by accessing the location (IP address) of the scheduler/master node at port `8787`.\n",
    "Using Docker with the appropriate port-mapping (provided in the Docker compose file) we can access the dashboard on `localhost:8787`.\n",
    "\n",
    "The dashboard provides an overview of both the status of the workers, and the execution of the DAG task graphs (when we begin using them)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple parallelization of python functions\n",
    "\n",
    "_In the following, we will follow a similar approach to the one provided by the excellent Dask documentation._\n",
    "\n",
    "The simplest example of parallelizing any arbitrary Python code in Dask can be demonstrated with a couple of simple operations, represented by two functions and an arbitrary sleep time of 1 second.\n",
    "\n",
    "_The sleep time is meant to represent arbitrarily complex code (___your___ task) and the time required for its execution._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "# dummy function incrementing the input value by 1\n",
    "def increment(x):\n",
    "    # sleep for 1s\n",
    "    sleep(1)\n",
    "    # given the input x, return x+1\n",
    "    return x + 1\n",
    "\n",
    "# dummy function decrementing the input value by 1\n",
    "def decrement(x):\n",
    "    # sleep for 1s\n",
    "    sleep(1)\n",
    "    # given the input x, return x-1\n",
    "    return x - 1\n",
    "\n",
    "# dummy function summing two input values\n",
    "def add(x, y):\n",
    "    # sleep for 1s\n",
    "    sleep(1)\n",
    "    # given the inputs x and y, return x+y\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are purely Python functions...\n",
    "\n",
    "We can test the functions locally by running them on the client (not on the cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# we should expect a wall-time of roughly 3 seconds (3x 1sec sleep)\n",
    "x = increment(1)\n",
    "y = decrement(2)\n",
    "z = add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to let Dask take advantage of the processing units assigned to the cluster, we have to let it elaborate the DAG corresponding to the execution of the code we want to deploy on the cluster, and then let the scheduler dispatch the tasks to the workers.\n",
    "\n",
    "This is done in Dask by declaring that a function is `delayed`.\n",
    "\n",
    "The `delayed` Dask method takes two main arguments. \n",
    "- The first argument is the function that has to be executed in parallel.\n",
    "- The following arguments are the arguments onto which the original function will apply.\n",
    "\n",
    "We now want to transform the `increment`, `decrement` and `add` functions, therefore making them **lazy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dask delayed module\n",
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# make the function behave as lazy with delayed\n",
    "#\n",
    "# result = delayed(your_function)(<your_function_arguments>)\n",
    "x = delayed(increment)(1)\n",
    "y = delayed(decrement)(2)\n",
    "z = delayed(add)(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, as usual with lazy operations, we don't have the results stored in `z`.\n",
    "\n",
    "`z` at this stage is just the \"plan\" of the code execution, made by the DAG task scheduler.\n",
    "\n",
    "We can visualize the plan of the execution with the `visualize()` method (we'll need to have the `graphviz` python package installed for this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask offers a nice and simple \n",
    "# visualization of the DAG\n",
    "z.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the visualization of the task graph **before** any degree of optimization is applied.\n",
    "\n",
    "We can also ask Dask to provide the DAG **after** the optimization, by using the option `optimize_graph=True`.\n",
    "\n",
    "We should not expect a lot of optimization for this simple task... but for more complex tasks, we will see a sizeable difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized DAG\n",
    "z.visualize(optimize_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, Dask also provides a high-level task visualization tool that can be accessed by clicking on it from a Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high-level task graph\n",
    "z.dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to **actually** execute the job, we have to ask Dask to trigger the execution, by asking to return the results of the lazy operation with the `compute()` method.\n",
    "\n",
    "Under the carpet, Dask will ship the computational graph to the scheduler, and will dispatch the tasks to the workers, similarly to what already discussed for Spark.\n",
    "\n",
    "It's worth noting that `compute()` is a *blocking* operation, which means that the program will wait for the results to be computed before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# the execution time should now be \n",
    "# less than the 3 seconds measured above\n",
    "# \n",
    "# ideally, we should expect 2 seconds:\n",
    "#  - 1 second (sleep time) to run the increment and decrement functions --in parallel--\n",
    "#  - 1 second (sleep time) to run the add function based on the results of the previous stages\n",
    "z.compute() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Futures and Eager computation with `submit`\n",
    "\n",
    "The _eager_ operation alternative to `delayed` in Dask is the `submit`.\n",
    "\n",
    "This will instruct Dask to submit our task on the cluster right away, and start executing it on the available computing resources.\n",
    "\n",
    "It is pretty much similar (almost equivalent even) what we have seen with the idea of a _batch system_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the function increment with argument 1\n",
    "# return a \"promise\" of getting the actual result\n",
    "future = client.submit(increment, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `increment` with its agurment `1` is submitted to the cluster, and the `future` object is returned immediately, but it doesn't contain the result of the computation. \n",
    "\n",
    "The `submit` operation has in fact returned a so-called execution ***promise*** (the `future` variable) of the instruction that we have submitted.\n",
    "It doesn't necessarily mean that the execution has been completed, as the cluster might take some time to execute it.\n",
    "\n",
    "The `future` variable in fact **doen not contains the result**, but just the promise of it, when the elaboration will be completed. \n",
    "\n",
    "**The result of the computation will be left on the worker nodes of our cluster, and will not be sent back to our client until we reclaim it.**\n",
    "\n",
    "This is extremely important (and useful) when the result of a computation is itself very large, and/or when you will need to execute further tasks on the data resulting from the results of the previous task. \n",
    "\n",
    "In the case we want to retrieve the result from the cluster to the client we can invoke the `gather` function on the `future` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the promise of the result\n",
    "future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result, gathered from the cluster\n",
    "client.gather(future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach can be extremely useful in situations where we need to submit a task multiple times, perhaps with different input parameters. \n",
    "\n",
    "This is a quite frequent scenario in machine learning, for example when we need to optimize an algorithm for a particular dataset over the hyper-parameter space. \n",
    "\n",
    "The idea behind this approach allows us to `map` the instruction that we want to execute to each argument of a dataset, thus submitting the same operation on the entire dataset.\n",
    "\n",
    "_Practically speaking, we are parallelizing our function in the same exact way we have done with the Python `multiprocessing` module, but on a large and horizontally scalable set of computing resources._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart from a list of integers\n",
    "data = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# submit the increment function on ALL elements of the list\n",
    "# using a `map` approach (each element is independent from the others)\n",
    "future_results = client.map(increment, data)\n",
    "\n",
    "# this execution is Eager and Asynchronous  \n",
    "future_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the data from the cluster\n",
    "new_data = client.gather(future_results)\n",
    "\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`client.map` is **asynchronous**, i.e. the cluster will compute the results without blocking our local Python code for the taks completion.\n",
    "\n",
    "_Remember the idea of `process.start()` and `process.join()` in multiprocessing..._\n",
    "\n",
    "**If** we need to wait for the result of a `submit` computation to be ready (for example, if we need it as an input for other computations), we can use the `wait` method to block the execution of the new code and wait for the computation of the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dask wait \n",
    "from dask.distributed import wait\n",
    "\n",
    "# start the computation\n",
    "new_future = client.map(increment, new_data)\n",
    "\n",
    "# block this python process \n",
    "# and wait for the remote task to be completed\n",
    "wait(new_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result, gathered from the cluster\n",
    "client.gather(new_future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In complete analogy to what we have discussed with the `delayed` lazy execution, we can combine multiple instruction that have to be submitted to the cluster and create a more complex job to run on our cluster.\n",
    "\n",
    "We should remember that the results of the `submit` execution reside on the cluster until a `gather` is used.\n",
    "\n",
    "This means that **we can submit a task that takes as argument an execution promise** of an instruction that has been previosly submitted.\n",
    "\n",
    "The `gather` function should be invoked at the end of the program, only when the results have to be effectively retrieved from the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the increment function\n",
    "x = client.submit(increment, 1)\n",
    "# submit the decrement function\n",
    "y = client.submit(decrement, 2)\n",
    "\n",
    "# submit the add function\n",
    "#\n",
    "# this will run on the promises of the results \n",
    "# of both the x and y (possibly not yet available)\n",
    "total = client.submit(add, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is still the execution promise\n",
    "print(total)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the final result\n",
    "client.gather(total)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Parallelization of a for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, with the previous knowledge of what discussed with Spark, and the `delayed` and `compute` Dask operations, we should already be able to run simple \"dummy\" tasks.\n",
    "\n",
    "Starting from a list $\\vec{x}$ of values:\n",
    "1. write a function to increment each element $x_i$ by a random value $\\delta x_i$ (in the 0-1 range)\n",
    "2. write a function to multiply the resulting value by 10\n",
    "3. loop over $\\vec{x}$ and apply both functions on each $x_i$ element using the `delayed`\n",
    "4. retrieve the sum of all the new updated elements\n",
    "5. visualize the task graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# input data\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# increment function\n",
    "def add_rand(x):\n",
    "    return x+random.random()\n",
    "\n",
    "# multiplication function\n",
    "def mult_ten(x):\n",
    "    return x*10\n",
    "\n",
    "# placeholder for the updated array\n",
    "results = []\n",
    "\n",
    "# for each element\n",
    "#  - add a random value\n",
    "#  - multiply by ten\n",
    "#  - append the new data in a list\n",
    "for x in data:\n",
    "    \n",
    "    \n",
    "# sum all elements of the list\n",
    "total = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the task graph\n",
    "total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the Optimized task graph\n",
    "# \n",
    "# once again, we should not expect any real optimization \n",
    "# to be viable for this map-like operations\n",
    "total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# compute the result and time it\n",
    "result = total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the result\n",
    "print(\"result: \",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the same task with the eager `submit` execution approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# placeholder for the updated array\n",
    "results = []\n",
    "\n",
    "# for each element\n",
    "#  - add a random value\n",
    "#  - multiply by ten\n",
    "#  - append the new data in a list\n",
    "for x in data:\n",
    "    \n",
    "# sum of all elements of the list\n",
    "total = client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the future object\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the result\n",
    "print(\"result: \",client.   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is clearly another alternative to the previous approach using the **map** Dask functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# map both functions on all data elements\n",
    "\n",
    "# submit the sum function on the z future\n",
    "total = client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the future object\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the result\n",
    "print(\"result: \",client.   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Map Reduce with Dask\n",
    "\n",
    "So far we acted mostly with _map_ like operations, and we collected all outputs in a single `sum` operation.\n",
    "\n",
    "We need to explore how we could write in Dask an equivalent _reduce_ function to evaluate the sum of a list of elements pair-wise.\n",
    "\n",
    "We can use the `add` function previously defined (including the 1 second sleep time) to visualize the time taken to run the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there is a pair-reduction algorithm implemented with (a _\"nasty\"_) nested `for` loop and a bit of simple Python logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "L = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "while len(L) > 1:\n",
    "    _ = []\n",
    "    for i in range(0, len(L), 2):\n",
    "        if i+1 < len(L):\n",
    "            pair_sum = add(L[i], L[i + 1]) \n",
    "        else:\n",
    "            pair_sum = add(L[i], 0)       \n",
    "        _.append(pair_sum)\n",
    "    L = _ \n",
    "    \n",
    "print(\"result:\",L[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parallelize this reduce task, we can define the pair-wise sum of neighbour elements as `delayed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "L = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# rewrite the same algorithm using delayed\n",
    "while len(L) > 1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the task graph for L[0]\n",
    "L[0].visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# compute the result\n",
    "result = L[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"result\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 : Count how many words are present in a series of documents \n",
    "\n",
    "Starting from a dataset composed of pieces of text taken from `sklearn`, count how many words are present in each document and calculate how many words are available in the total dataset. \n",
    "\n",
    "The documents are ~8000.\n",
    "\n",
    "One way to proceed would be to loop over all documents, and count over all words.\n",
    "However, we should be now smarter than this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from dask.distributed import Client\n",
    "import time\n",
    "\n",
    "categories = [\n",
    "     'comp.graphics',\n",
    "     'comp.os.ms-windows.misc',\n",
    "     'comp.sys.ibm.pc.hardware',\n",
    "     'comp.sys.mac.hardware',\n",
    "     'comp.windows.x',\n",
    "     'misc.forsale',\n",
    "     'rec.autos',\n",
    "     'rec.motorcycles',\n",
    "     'rec.sport.baseball',\n",
    "     'rec.sport.hockey',\n",
    "     'sci.crypt',\n",
    "     'sci.electronics',\n",
    "     'sci.med',\n",
    "     'sci.space'\n",
    "]\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='train', categories=categories ).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Documents in the dataset:\",len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple function to split a body of text \n",
    "# into words and count them\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example of single-threaded execution in plain Python can be the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# initialize a word-per-document list\n",
    "total_words_per_document = []\n",
    "\n",
    "# count the number of words in each document\n",
    "for document in dataset:\n",
    "    total_words_per_document.append(count_words(document))\n",
    "\n",
    "# calculate the total number of words in the dataset\n",
    "total_words_dataset = sum(total_words_per_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of words in the dataset: {total_words_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(total_words_per_document,bins=range(0,1000,10));\n",
    "plt.xlabel('words per document');\n",
    "plt.ylabel('counts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the distributed version using the `delayed` lazy execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# initialize a word-per-document list\n",
    "\n",
    "\n",
    "# count the number of words in each document\n",
    "# using dask delayed \n",
    "\n",
    "\n",
    "# calculate the total number of words in the dataset\n",
    "# using dask delayed \n",
    "total_words_lazy = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# execute the tasks and retrieve the result\n",
    "lazy_result = \n",
    "\n",
    "print(f\"Total number of words in the dataset: {lazy_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the distributed version using the `map` and `submit` eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# count the number of words in each document\n",
    "# using dask map \n",
    "\n",
    "\n",
    "# calculate the total number of words in the dataset\n",
    "# using dask submit \n",
    "total_words_eager = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the future object\n",
    "total_words_eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eager_result = \n",
    "print(\"Total number of words in the dataset: {}\".format(eager_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Calculate the first $n$ Fibonacci numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the plain Python algorithm to evaluate the sequence of Fibonacci up to the $n$-th element:\n",
    "\n",
    "Once again, let's start from some simple single-threaded Python code for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci_sequential(num):\n",
    "    iteration = 1\n",
    "    fibonacci = []\n",
    "    if num <= 0:\n",
    "        pass\n",
    "    elif num == 1:\n",
    "        fibonacci.append(1)\n",
    "    elif num == 2:\n",
    "        fibonacci.append(1)\n",
    "        fibonacci.append(1)\n",
    "    elif num > 2:\n",
    "        fibonacci.append(1)\n",
    "        fibonacci.append(1)\n",
    "        while iteration < (num - 1):\n",
    "            fibonacci.append(fibonacci[iteration] + fibonacci[iteration-1])\n",
    "            iteration+=1\n",
    "    return fibonacci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "n = 17\n",
    "\n",
    "print(f\"The first {n} fibonacci numbers are: {fibonacci_sequential(n)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the same algorithm using the Dask lazy evaluation, with `delayed`, and inspect the task graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to generalize the append\n",
    "def append(arr = [], val = 0):\n",
    "    if val != None:\n",
    "        arr.append(val)\n",
    "    return arr\n",
    "\n",
    "def fibonacci_delayed(num):\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "result = fibonacci_delayed(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the graph\n",
    "result.visualize(rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute\n",
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Monte Carlo Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we want to integrate of a function via MonteCarlo technique, as you have discussed in LCP Module A.\n",
    "\n",
    "Let's use the function $$f(x) =\\sin^2{\\left(\\frac{1}{x(2-x)}\\right)}$$ and let's integrate in the range $(0,2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return (np.sin(1/(x*(2-x))))**2\n",
    "\n",
    "x=np.linspace(-0.2,2.2,1000)\n",
    "\n",
    "plt.figure(figsize=(16,6));\n",
    "plt.plot(x,f(x),'grey','.');\n",
    "plt.fill_between(x[np.where((x>0) & (x<2))],[1]*len(np.where((x>0) & (x<2))), alpha=0.2);\n",
    "plt.fill_between(x[np.where((x>0) & (x<2))],f(x[np.where((x>0) & (x<2))]), alpha=0.2);\n",
    "plt.vlines([0, 2], 0, 1, colors = [\"k\", \"k\"], linestyles = [\"dashed\", \"dashed\"],linewidths=[3,3],zorder=20);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('$f(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the single-thread Python code to execute this task, and evaluate the integral over N=100 000 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Monte Carlo integration using Python's built-in random module\n",
    "\n",
    "# number of points to use for integration\n",
    "N = 100000\n",
    "\n",
    "# list of counts for points that pass the function test\n",
    "count = []\n",
    "\n",
    "# function to check if a random point is passing\n",
    "def pass_function():\n",
    "    # generate a random x value between 0 and 2\n",
    "    # generate a random y value between 0 and 1\n",
    "    x = 2 * random.random()\n",
    "    y = random.random()\n",
    "    # check if the point (x, y) is under the curve defined by f(x)\n",
    "    return 1 if y < f(x) else 0\n",
    "\n",
    "# iterate over all points and count how many pass the function test\n",
    "for i in range(N):\n",
    "    count.append(pass_function())\n",
    "\n",
    "# compute the integral by dividing the sum of the counts by the total number of points,\n",
    "# and multiply by the width of the integration range (2)\n",
    "I = 2 * sum(count) / N\n",
    "\n",
    "# print the result\n",
    "print(f\"Integral = {I}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the minimal changes to the code to deploy the same integral computation on the Dask cluster using `delayed`\n",
    "\n",
    "**NOTE**: Do NOT use 100 000 points in this case, but ___limit the computation to N=1 000 points___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Monte Carlo integration using Python's built-in random module\n",
    "\n",
    "# number of points to use for integration\n",
    "N = 1_000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even use Python **decorators** to declare that an entire function is going to be interpreted as `delayed`\n",
    "\n",
    "```python\n",
    "@dask.delayed\n",
    "def my_delayed_function(arg):\n",
    "    ...\n",
    "    return val\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Monte Carlo integration using Python's built-in random module\n",
    "\n",
    "# import dask to use its delayed decorator @dask.delayed\n",
    "import dask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why we are getting worse performance with respect to the single-threaded execution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dask code is slower than the plain Python code because of the **overhead** introduced by Dask itself. \n",
    "\n",
    "When we use Dask, we are creating a task graph that Dask needs to traverse in order to execute the computation. This involves additional computation and communication overhead between the scheduler and the workers, which can add some extra time compared to the plain Python version.\n",
    "\n",
    "_Every delayed task per se has an overhead of a hundreds of microseconds. If thousands of computations are issued, the overhead can be substantial._\n",
    "\n",
    "Additionally, we are using `delayed` to parallelize the computation in a way that doesn't take full advantage of Dask's parallelism capabilities. \n",
    "\n",
    "Specifically, we are creating a list of `delayed` objects and then computing them all at once using `delayed(sum)(count)`. 1000 `count` functions will be have to be executed as individual processes on the worker nodes.\n",
    "\n",
    "This can be inefficient, as it's still running the loop in sequence and then aggregating the results in parallel. Instead, you could use Dask's `dask.bag` to parallelize the computation more effectively. \n",
    "\n",
    "Overall, when using Dask, it's important to make sure we're taking full advantage of its capabilities to parallelize the computation. In some cases, using Dask may not result in any performance gains and can even be slower than plain Python, if we don't take the time to rethink the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: $\\pi$ via MonteCarlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can re-run the evaluation of $\\pi$ using the Monte Carlo technique as done during the Spark hands-on session. \n",
    "\n",
    "However, please note that so far with Dask, we are not creating data partitions yet. Instead, we are instructing Dask to run a simple task for each entry in our list, which results in a strong overhead.\n",
    "\n",
    "Therefore, it's recommended to use a very limited number of points (start with around 10 and a maximum of around 100), and check the status of the job from the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "\n",
    "# set the number of points to use\n",
    "num_points = 100\n",
    "\n",
    "# list to hold the points inside the circle\n",
    "points_in_circle = []\n",
    "\n",
    "# function to check if a point is inside the circle\n",
    "def in_circle():\n",
    "\n",
    "    \n",
    "    \n",
    "# iterate over all the points and count how many are inside the circle\n",
    "\n",
    "\n",
    "\n",
    "# count the number of points inside the circle\n",
    "num_points_inside = \n",
    "\n",
    "\n",
    "# compute pi\n",
    "pi = 4*num_points_inside/num_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print result\n",
    "print (f\"pi = {pi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `delayed` to parallelize the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print result\n",
    "print (f\"pi = {pi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `map` or `submit` to parallelize the computation using the Dask eager execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `submit` we have to be very careful...\n",
    "\n",
    "Calling naively `client.map` with a function returning a random variable would imply returning all the times the same exact random value (we have _1 function_ executed 10000 times).\n",
    "\n",
    "We can however instruct Dask that this function is in fact `inpure`.\n",
    "\n",
    "```python\n",
    "client.submit(your_function, pure=False)\n",
    "```\n",
    "\n",
    "An inpure function can return different output values for the same input.\n",
    "\n",
    "This will guarantee us to get 10000 different random variables, but can be problematic for parallelization for Dask, as we can end up with unnecessary overhead and performance penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print result\n",
    "print (f\"pi = {pi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take-away message\n",
    "\n",
    "Dask is a flexible tool allowing for the lazy or eager scheduling of our pure Pythonic code and tasks.\n",
    "\n",
    "However, as we discussed, in general parallelization can be beneficial when the task involves a large amount of data or computationally intensive operations that can be split up into smaller chunks that can be executed concurrently. \n",
    "\n",
    "The simple functions used in this case are definitely not a good representation of a \"heavy task\".\n",
    "\n",
    "Additionally, Dask Distributed can provide performance benefits when the computation is distributed across multiple nodes or machines, rather than just across a few CPU cores on a single machine (like in our case using Docker compose).\n",
    "\n",
    "For all these reasons, it's always important to carefully evaluate the performance of our parallelized code and compare it to the performance of the single-threaded code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use `docker compose down` to stop and clear all running containers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
