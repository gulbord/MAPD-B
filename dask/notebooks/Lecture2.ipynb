{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f76699",
   "metadata": {},
   "source": [
    "# Dask Data structures\n",
    "\n",
    "Dask offers several pythonic data structures to handle and operate with larger-than-memory data in a distributed system.\n",
    "- `dask.bag`: distributed generic python list. The Dask equivalent to a PySpark RDD\n",
    "- `dask.array`: distributed numpy arrays\n",
    "- `dask.dataframe`: distributed pandas dataframes\n",
    "\n",
    "All the high-level data structure APIs are optimized to exploit the DAG optimization features of the Dask scheduler, and thus rely on lazy computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21d093",
   "metadata": {},
   "source": [
    "## Start the Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf23cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    " \n",
    "# use the provided master\n",
    "client = Client('dask-scheduler:8786')\n",
    "    \n",
    "# print the status of the client    \n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08409bcd",
   "metadata": {},
   "source": [
    "## Dask Bag\n",
    "\n",
    "Bags are very powerful and flexible data structures.\n",
    "The Dask Bag offers essentially the same degree of flexibility as the RDD in PySpark.\n",
    "They are parallelized general collections of objects, like Pythonâ€™s built-in `list`, and can therefore hold any Python objects, whether they are custom classes or built-in types. \n",
    "This makes it possible to contain very complicated data structures, like raw text or nested JSON data, and navigate them with ease.\n",
    "\n",
    "For these reasons, Dask bags are often used to parallelize simple computations on unstructured or semi-structured data such as text data, log files, JSON records, or user-defined Python objects, using MapReduce-like approaches to load, inspect, filter, and process arbitrary datasets (structured or unstructured).\n",
    "\n",
    "Dask Bag implements operations like `map`, `filter`, `groupby`, and aggregations on collections of Python objects.\n",
    "It does this in parallel using Python iterators, similar to a parallel version of itertools.\n",
    "\n",
    "Once a first stage of data preparation is completed using Dask Bag, it is quite common to reduce and convert the data into more suitable data structures, such as Dask Arrays or Dask DataFrames, which will be covered later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd63846",
   "metadata": {},
   "source": [
    "### Create and Take from a Bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced8b7d",
   "metadata": {},
   "source": [
    "We can create a `Bag` from a Python sequence, from files, or from data on cloud storage such as Amazon AWS S3, and more.\n",
    "For a comprehensive overview of the ways to access remote data from DFS, S3, and others, please refer to the official documentation at the [link](https://docs.dask.org/en/stable/how-to/connect-to-remote-data.html).\n",
    "\n",
    "We can also create a Bag from a function declared as `delayed`.\n",
    "This way, we can generate data from a distributed application and then access the data with the Bag API before computing a result.\n",
    "\n",
    "The data is partitioned into blocks, usually with multiple items per block, depending on the datasets, the cluster resources, and our choice of the `npartitions` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba350e",
   "metadata": {},
   "source": [
    "Let's start by creating some simple data from a Python list.\n",
    "Clearly, as Python is a dynamically typed language, this can be a simple array of integers or an arbitrary collection of multiple data types (numbers, strings, objects, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb8a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "\n",
    "# create a Dask Bag from a Python sequence\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# create a Dask Bag from the sequence with 4 partitions\n",
    "b = db.from_sequence(data, npartitions=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2905079b",
   "metadata": {},
   "source": [
    "As previously mentioned, Dask data structures embody the lazy programming paradigm.\n",
    "The data is thus not yet stored on the cluster, as we have not performed an operation such as `compute`.\n",
    "\n",
    "In general, we don't want to retrieve the entire data stored on the cluster, but we might want to inspect a few elements.\n",
    "We can do that with the `take(n_elements)` method.\n",
    "The returned data will be a tuple containing the first `n_elements` of the Bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49904962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the first 3 elements of the Bag\n",
    "b.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6266bd9c",
   "metadata": {},
   "source": [
    "Data can be extracted from text files by providing a list of all files or using the `*` wildcard.\n",
    "\n",
    "By default, the resulting Bag will have one item per line and one file per partition (so be careful when partitioning the data).\n",
    "\n",
    "A nice feature of Dask when reading text files is that it handles standard compression libraries (like gzip, bz2, xz) automatically. The compression can be inferred from the file name extension or specified using the `compression='gzip'` keyword argument.\n",
    "\n",
    "For instance, we can load a number of compressed files from a local folder into a Bag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d025db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the files provided\n",
    "! ls datasets/accounts_json/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a3f608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# read text files from the specified directory and use gzip compression\n",
    "b = db.read_text(os.path.join('datasets','accounts_json','accounts.*.json.gz'),\n",
    "                 files_per_partition=4)\n",
    "\n",
    "# take the first element from the bag\n",
    "example = b.take(1)\n",
    "\n",
    "# print the type of the example variable\n",
    "print(type(example))\n",
    "print()\n",
    "\n",
    "# print the example variable\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce9554e",
   "metadata": {},
   "source": [
    "`Bag` objects support standard functional APIs, including `map`, `filter`, `groupby`, and more.\n",
    "\n",
    "Operations on `Bag` objects create new bags, allowing us to chain multiple operations together to manipulate the data until we obtain the desired result.\n",
    "\n",
    "To trigger the execution, we can use the `.compute()` method, similar to any `delayed` object we've seen before.\n",
    "\n",
    "Since a bag is inherently a delayed object, there is no need to explicitly specify that the functions we apply to the dataset are further delayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check if a number is even\n",
    "def is_even(n):\n",
    "    return n % 2 == 0\n",
    "\n",
    "# create a bag from a sequence of numbers\n",
    "b = db.from_sequence([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# filter the numbers to keep only the even ones\n",
    "# then, map a lambda function to square each even number\n",
    "c = b.filter(is_even).map(lambda x: x ** 2)\n",
    "\n",
    "# print the resulting bag\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7706d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the computational graph\n",
    "c.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabbc743",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute the results\n",
    "c.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9eb74f",
   "metadata": {},
   "source": [
    "Please note that one of the crucial parameters when optimizing a computing task on a cluster is to ensure the effective utilization of available computing resources. This involves rewriting the computational task to be distributed efficiently and optimizing the number of partitions used to store data and perform map-like data transformations.\n",
    "\n",
    "Consider the following scenarios:\n",
    "- Having only one partition with 100 available CPUs would be inefficient as there would be no parallelization at all.\n",
    "- Having 10,000 tiny partitions with 3 available CPUs each would likely be inefficient due to the overhead of starting and stopping computation on each individual tiny partition.\n",
    "- The optimal number of partitions depends on factors such as the amount of shuffling required (e.g., for `groupby` operations). For example, having 100 partitions with 20 available CPUs might be more efficient than having 25 partitions, but the optimal choice will depend on your specific workload.\n",
    "\n",
    "Determining the optimal partitioning strategy is a task that requires optimization and educated guesses based on the available processing units in your cluster. There is no one-size-fits-all answer to this question.\n",
    "\n",
    "Ultimately, you will need to experiment and fine-tune your partitioning strategy based on your specific workload and cluster characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f17b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new bag with the same data, but a different number of partitions\n",
    "b = db.from_sequence([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], npartitions=3)\n",
    "\n",
    "# filter the numbers to keep only the even ones\n",
    "# then, map a lambda function to square each even number\n",
    "c = b.filter(is_even).map(lambda x: x ** 2)\n",
    "\n",
    "# visualize the computation graph\n",
    "c.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f9f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the results\n",
    "c.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c66468",
   "metadata": {},
   "source": [
    "## Exercise 1 - Open and preprocess JSON data\n",
    "\n",
    "We'll start with a dummy dataset of gzipped JSON data in your data directory. This dataset is analogous to what you might collect from a document store database (e.g., MongoDB) or by scraping a website using the dedicated API.\n",
    "\n",
    "Each line of each document is a JSON-encoded dictionary with the following keys:\n",
    "\n",
    "* `id`: Unique identifier of the customer\n",
    "* `name`: Name of the customer\n",
    "* `transactions`: A list of key-value pairs in the form of `transaction-id` and `amount` pairs (one for each transaction made by the customer in that file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07021218",
   "metadata": {},
   "source": [
    "1. **Create a Bag reading out the dataset from the text files**\n",
    "2. **Map the `json.loads` function on each message to extract the data in the form of python dictionaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e136dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create a Dask Bag `db_lines` from the files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. read the data from the JSON format into a `db_js` bag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9df035",
   "metadata": {},
   "source": [
    "Once the JSON data is mapped into the appropriate Python objects (dictionaries, lists, etc.), we can perform specific operations by creating small Python functions to run on our data.\n",
    "\n",
    "The most basic operations we can perform on a Dask Bag are as follows:\n",
    "- `map`: Apply a function to each element.\n",
    "- `filter`: Retain only the elements that satisfy a given function.\n",
    "- `pluck`: Select a specific nested field, such as `element[field]` from a Python dictionary.\n",
    "- `flatten`: Unfold the dictionary into a list-like object.\n",
    "\n",
    "These operations provide a foundation for manipulating and transforming the data within a Dask Bag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42fc7fe",
   "metadata": {},
   "source": [
    "**1. compute the average number of transactions for each entry of a user named \"Alice\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f076cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the records from users named \"Alice\"\n",
    "db_js."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a84b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the records from users named \"Alice\"\n",
    "# AND count the total number of transactions for each entry in the dataset \n",
    "\n",
    "# function reformatting each record with the new information\n",
    "def count_transactions(d):\n",
    "    return \n",
    "\n",
    "db_js."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the records from users named \"Alice\"\n",
    "# AND count the total number of transactions (as 'count') for each entry in the dataset \n",
    "# AND return only the 'count' values\n",
    "db_js."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f06cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the records from users named \"Alice\"\n",
    "# AND count the total number of transactions (as 'count') for each entry in the dataset \n",
    "# AND return only the 'count' values\n",
    "# AND compute the average of the counts\n",
    "db_js."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc7b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the graph of the tasks composing the job\n",
    "db_js."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc0880",
   "metadata": {},
   "source": [
    "**2. compute the average amount for all transactions for all users named \"Alice\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb8bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the records from users named \"Alice\"\n",
    "db_js."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333441ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the records from users named \"Alice\"\n",
    "# AND flatten and pluck to return only the \"amount\" in a bag\n",
    "db_js."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d440031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the relevant transactions\n",
    "# AND flatten and pluck to return only the \"amount\" in a bag\n",
    "# AND compute the average of all transactions amounts\n",
    "db_js."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1be15df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the graph of the tasks composing the job\n",
    "db_js."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb1002",
   "metadata": {},
   "source": [
    "Additional standard operations on Dask Bags can be performed using **groupby** and **aggregation functions**.\n",
    "\n",
    "- `groupby`: Shuffles the data so that all items with the same key are in the same key-value pair.\n",
    "- `foldby`: Walks through the data and accumulates a result per key. It combines the functionality of `groupby` and reduce operation, making it suitable for efficient parallel split-apply-combine tasks.\n",
    "\n",
    "However, it's important to note that any operation that involves heavy data shuffling, such as `groupby`, is computationally expensive as it requires moving data across workers. In such cases, the `foldby` method in Dask provides a more efficient alternative. While `foldby` is more complex to use, it significantly reduces the computational time required.\n",
    "\n",
    "Consider using the `foldby` method whenever possible to optimize performance and minimize data shuffling overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461dc7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_data = ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith', 'Frank']\n",
    "\n",
    "# create a bag from the list of names\n",
    "b = db.from_sequence(names_data)\n",
    "\n",
    "# group names by length\n",
    "res = b.groupby(len) \n",
    "\n",
    "# visualize this \"simple\" graph before computing the results\n",
    "res.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f646a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the results\n",
    "res.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a261a",
   "metadata": {},
   "source": [
    "Notice that the result of the `groupby` operation is a tuple. If we need to apply functions to the elements of these tuples, we can use `starmap`.\n",
    "\n",
    "The `starmap` function in Dask allows us to apply a function using argument tuples, similar to what the standard `itertools.starmap` does in Python.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07a4703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple bag from a list of integers\n",
    "b = db.from_sequence(list(range(10)))\n",
    "\n",
    "# group numbers into even/odd groups\n",
    "b.groupby(lambda x: x % 2).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8199de8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the max value for all elements in each group\n",
    "b.groupby(lambda x: x % 2)\\\n",
    " .starmap(lambda k, v: (k, max(v)))\\\n",
    " .compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec01c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the sum of the elements in each group\n",
    "b.groupby(lambda x: x % 2)\\\n",
    " .starmap(lambda k, v: (k, sum(v)))\\\n",
    " .compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9450214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the graph of this latest \"extremely simple\" computation\n",
    "res = b.groupby(lambda x: x % 2)\\\n",
    "       .starmap(lambda k, v: (k, sum(v)))\n",
    "\n",
    "res.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9a221",
   "metadata": {},
   "source": [
    "`foldby` can seem quite peculiar at first, but it is similar to the following functions in other libraries:\n",
    "\n",
    "- [`toolz.reduceby`](http://toolz.readthedocs.io/en/latest/streaming-analytics.html#streaming-split-apply-combine)\n",
    "- [`pyspark.RDD.combineByKey`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.combineByKey.html)\n",
    "\n",
    "When using `foldby`, you need to provide:\n",
    "\n",
    "1. A **key function to group elements** (similar to `groupby`)\n",
    "2. A **binary operator** (function that takes 2 elements and return 1 of the same type) that performs **reduction within each group**\n",
    "3. A **combine binary operator** that can **combine the results of two `reduce` calls on different parts of your dataset**\n",
    "\n",
    "In Dask, a `foldby` call like this:\n",
    "```python\n",
    "dask_bag.foldby(key, binop, init)\n",
    "```\n",
    "\n",
    "is equivalent to a combination of two operations: groupby and reduce:\n",
    "\n",
    "```python\n",
    "def reduction(group):\n",
    "    return reduce(binop, group, init)\n",
    "\n",
    "dask_bag.groupby(key).map(lambda (k, v): (k, reduction(v)))\n",
    "```\n",
    "\n",
    "The reduction operation must be associative, and it is executed in parallel within each partition of the dataset. The intermediate results are then combined using the `combine` binary operator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1117495d",
   "metadata": {},
   "source": [
    "Let's re-write the equivalent group-by + starmap operation with a foldby call\n",
    "\n",
    "```python\n",
    "b.groupby(lambda x: x % 2)\\\n",
    " .starmap(lambda k, v: (k, sum(v)))\\\n",
    " .compute()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple bag from a list of integers\n",
    "b = db.from_sequence(list(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eaf754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby even/odd numbers with a foldby and find the total sum per group\n",
    "#\n",
    "#   write down a **binary filter function** to select only even or odd numbers\n",
    "#   write down a **reduce-like operation** to sum all elements\n",
    "is_even = lambda x: x % 2 == 0 \n",
    "add     = lambda x, y: x + y\n",
    "b.foldby(key=is_even, \n",
    "         binop=add, \n",
    "         initial=0).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b20e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the graph and compare it \n",
    "# with the groupby implementation\n",
    "# \n",
    "# the `split_every` option instructs foldby to group \n",
    "# partitions into groups of this size while performing the reduction.\n",
    "# (`split_every` defaults to 8)\n",
    "b.foldby(is_even, \n",
    "         add, \n",
    "         initial=0, \n",
    "         split_every=8).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b5dc49",
   "metadata": {},
   "source": [
    "## Exercise 2 - Account data\n",
    "\n",
    "Take a moment to look to the `foldby` API at the [link](https://docs.dask.org/en/latest/generated/dask.bag.Bag.foldby.html#dask.bag.Bag.foldby).\n",
    "\n",
    "- Get the total number of users with the same name from the account dataset\n",
    "  1. Use a `groupby` function and measure the required computational time\n",
    "  2. Use a `foldby` function and measure the required computational time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b803d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# groupby on the 'name' key \n",
    "# count the number of items in each group\n",
    "result_groupby = db_js."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9b4bcd",
   "metadata": {},
   "source": [
    "Let's inspect what `groupby` is doing in Dask..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1a3cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(result_groupby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cbafd2",
   "metadata": {},
   "source": [
    "**Wait a second...!**\n",
    "\n",
    "What about `foldby` then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3268862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# foldby on 'name' key \n",
    "# increment by one each time we see an element (binop function)\n",
    "# use a final combination function with add (combine function)\n",
    "result_foldby = db_js."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef768fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(result_foldby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0a137",
   "metadata": {},
   "source": [
    "Dask may exhibit unexpected behavior when using the `groupby` function with string keys instead of numerical features. In some cases, it may return a group count per partition instead of a single group count per key.\n",
    "\n",
    "This issue highlights a couple of factors:\n",
    "1. Dask is a relatively new framework and is still being actively developed.\n",
    "2. Distributed computing, especially when dealing with complex operations like grouping, can be challenging to implement correctly and efficiently.\n",
    "3. In Dask, the `foldby` function is often preferred over `groupby` as it provides more control and efficiency in the grouping and reduction process.\n",
    "\n",
    "If you encounter issues with `groupby` and string keys, it is recommended to consider alternative approaches or use `foldby` for more reliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd7bee",
   "metadata": {},
   "source": [
    "## Exercise 3 - Still on account data\n",
    "\n",
    "Let's compute the total transfers amount per each name using a foldby operation.\n",
    "\n",
    "We can proceed in two steps:\n",
    "1.  Create a function that takes an input dictionary, such as:\n",
    "    ```python\n",
    "        {'name': 'Alice', 'transactions': [{'amount': 1, 'id': 123}, {'amount': 2, 'id': 456}]}\n",
    "    ``` \n",
    "    and produces the sum of the amounts. For example, in this case, the sum would be 3.\n",
    "    \n",
    "2.  Modify the binary operator of the `foldby` examples above so that it accumulates the sum of the transferred amounts instead of counting the number of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ab46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the sum_transactions values together\n",
    "def add_values(tot, _):\n",
    "    return \n",
    "\n",
    "# compute the sum of transaction amounts per name\n",
    "def sum_amount(d):    \n",
    "    return \n",
    "\n",
    "# apply sum_amount function to each item in db_js\n",
    "# perform foldby operation on 'name' key to accumulate the sum of transactions for each name\n",
    "result_foldby = db_js."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef61681",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_foldby.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result_foldby = result_foldby.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19245641",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(result_foldby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a302c",
   "metadata": {},
   "source": [
    "## From Bag to pre-processed output datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81c10ab",
   "metadata": {},
   "source": [
    "Dask Bags are often used as an entry point to ingest, decode, and preprocess data before further processing or analysis. Once the data has been processed and transformed, it is often desirable to convert the Dask Bag into a structured data format, such as a Dask DataFrame, for easier manipulation and analysis.\n",
    "\n",
    "Dask provides several methods to convert Bags into output data objects, including text files, JSON files, and more.\n",
    "You can refer to the documentation at the [link](https://docs.dask.org/en/stable/bag-creation.html#store-dask-bags) to learn more about these methods:\n",
    "- `to_textfiles`: writes the Bag data to multiple text files.\n",
    "- `to_avro`: writes the Bag data to Avro files.\n",
    "- `to_delayed`: converts the Bag into a list of delayed objects.\n",
    "\n",
    "By far the most widely used approach in data pre-processing using Dask Bags is to `extract` some raw data from the original input source, `transform` it applying some funcions to filter/reduce/create features from the original (usually messy) dataset, and finally `load` the clensed dataset into either a DataBase or a further data processing pipeline based on *structured* data.\n",
    "\n",
    "Converting a Dask Bag to a Dask Dataframe is thus a very common operation (very similar to the conversion from RDD to a Spark DataFrame).\n",
    "\n",
    "To convert a Dask Bag into a Dask DataFrame, the dataset needs to be flattened and normalized before invoking the `to_dataframe` function on the Bag. This flattening and normalization process ensures that the data is in a structured format that can be easily loaded into a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b9cdd",
   "metadata": {},
   "source": [
    "As a purely illustrative example, our account data is deeply nested and not suitable for being transformed into a table-like DataFrame structure.\n",
    "\n",
    "Assuming we may want to retain only the first transaction per customer (or alternatively, we might have wanted to retain other features, such as the max-amount transfer, or an aggregated quantity per user, etc), we can flatten the dataset by mapping a dedicated function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4964a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print one element of the json bag\n",
    "pprint( db_js.take(1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f4b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to flatten the nested structure of the record\n",
    "# and extract specific fields\n",
    "def dummy_flatten(record):\n",
    "    return {\n",
    "        'id': record['id'],\n",
    "        'name': record['name'],\n",
    "        'first_transaction_id': record['transactions'][0]['transaction-id'],\n",
    "        'first_transaction_amount': record['transactions'][0]['amount']\n",
    "    }\n",
    "\n",
    "# apply the dummy_flatten function to each record in db_js\n",
    "# and take the first element from the resulting Bag\n",
    "pprint(db_js.map(dummy_flatten).take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b3d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Dask DataFrame from the flattened Bag\n",
    "dd = db_js.map(dummy_flatten).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70535f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Dask DataFrame is still a Dask distributed object, i.e.:\n",
    "# 1. it is partitioned by default\n",
    "# 2. it is lazy\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb83c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# showing a number of elements (i.e. issuing the `head` method) \n",
    "# will trigger the computation and retrieve the results from the cluster\n",
    "dd.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac68dc",
   "metadata": {},
   "source": [
    "## Stop client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2045177d",
   "metadata": {},
   "source": [
    "Finally, use `docker compose down` to stop and clear all running containers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
